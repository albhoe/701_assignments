{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"assignment4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Clustering in Practice and K-Means Clustering\n",
    "\n",
    "Please complete the following assignment. Run all cells and submit your completed notebook through Gradescope.\n",
    "\n",
    "- Clustering is a fundamental unsupervised learning technique used to group similar data points based on their features. It helps in discovering patterns and structures within datasets without predefined labels.\n",
    "\n",
    "- The k-means algorithm aims to partition a dataset into k clusters, assigning each data point to the cluster whose centroid is closest to that point.\n",
    "\n",
    "**In this assignment, the aim is to check and reinforce your understanding  of clustering, specifically K-Means clustering.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Do not change any of the parameters given in the starter code (e.g., random_state=42, number of samples, centers, or cluster standard deviation).  \n",
    "\n",
    "These fixed values ensure your results are reproducible and match the expected test cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 1. Which of the following best describes the objective function minimized in k-means clustering?\n",
    "\n",
    "A) The variance between cluster centroids.\n",
    "\n",
    "B) The sum of squared distances between all pairs of data points.\n",
    "\n",
    "C) The sum of squared distances between each point and its assigned cluster centroid.\n",
    "\n",
    "D) The determinant of the covariance matrix of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 2. Which of the following is a known limitation of k-means?\n",
    "\n",
    "A) It cannot be implemented in higher dimensions.\n",
    "\n",
    "B) It performs poorly when clusters have non-spherical shapes.\n",
    "\n",
    "C) It can only work with datasets having fewer than 1,000 points.\n",
    "\n",
    "D) It guarantees the global optimum clustering solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 3. What is the main advantage of using the k-means++ initialization method over the standard k-means initialization?\n",
    "\n",
    "A) It selects initial centroids that are maximally distant from each other, ensuring convergence to the global optimum.\n",
    "\n",
    "B) It reduces the computational complexity of k-means from quadratic to linear time with respect to the number of data points.\n",
    "\n",
    "C) It allows k-means clustering to automatically determine the optimal number of clusters without prior specification.\n",
    "\n",
    "D) It probabilistically selects initial centroids based on data density, leading to faster convergence and improved clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### 4-a. Clustering on Isotropic Gaussian Distributed Data\n",
    "\n",
    "- A synthetic dataset has been generated using sklearn’s `make_blobs`.  \n",
    "- Apply **K-Means clustering** with `k = 3`.  \n",
    "- Plot the clustered data and highlight the cluster centers with a distinct marker.  \n",
    "- Return:  \n",
    "  - The cluster centers  \n",
    "  - The predicted cluster labels  \n",
    "  - The Within-Cluster Sum of Squares (WCSS)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def ans4a():\n",
    "    X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
    "    ...\n",
    "\n",
    "    # Return the following values:\n",
    "    # - The cluster centers\n",
    "    # - The predicted cluster labels\n",
    "    # - The Within-Cluster Sum of Squares (WCSS)\n",
    "    # - The original data points\n",
    "    return centers, y_kmeans, inertia, X\n",
    "\n",
    "\n",
    "centers, y_kmeans, inertia, X = ans4a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 4-b. Plot the original data points and the clustered data points\n",
    " \n",
    "- Plot the clustered data and highlight the cluster centers with a distinct marker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### 5a. Clustering on funky-looking anisotropic data\n",
    "\n",
    "- A synthetic dataset has been generated using sklearn’s `make_moons`.  \n",
    "- Apply **K-Means clustering** with `k = 2`.  \n",
    "- Return:  \n",
    "  - The predicted cluster labels  \n",
    "  - The true labels  \n",
    "  - The Adjusted Rand Index (ARI)\n",
    "  - The original data points\n",
    "  - The true labels\n",
    "  - The cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def ans5a():\n",
    "    # Generate noisy moons dataset\n",
    "    X, y_true = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
    "    ...\n",
    "\n",
    "    # Return the following values:\n",
    "    # - The predicted cluster labels\n",
    "    # - The true labels\n",
    "    # - The Adjusted Rand Index (ARI)\n",
    "    # - The original data points\n",
    "    # - The true labels\n",
    "    # - The cluster centers\n",
    "    return y_kmeans, y_true, ari, X, y_true, centers_\n",
    "\n",
    "\n",
    "y_kmeans, y_true, ari, X, y_true, centers = ans5a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 5-b. Plot the clustered data\n",
    "\n",
    "- Make two plots:  \n",
    "  - Plot the original data points.\n",
    "  - Plot the clustered data and highlight the cluster centers with a distinct marker.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the original, ground truth data points.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Now plot the k-means clustering results with the cluster centers highlighted.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### 6-a. Comparing K-Means Initialization Methods\n",
    "\n",
    "- In this question, you will **compare** K-Means clustering using **random initialization** and **k-means++ initialization** on synthetic data generated with `make_blobs`.  \n",
    "- Apply K-Means with the same number of clusters for both initialization methods.\n",
    "- Compare the two approaches by reporting:  \n",
    "  - The number of iterations until convergence  \n",
    "  - The WCSS (within-cluster sum of squares)    \n",
    "- Return the following values:\n",
    "  - `n_iter_random`, `n_iter_kpp`, `wcss_random`, `wcss_kpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def ans6a():\n",
    "    # Generated synthetic dataset with 5 clusters\n",
    "    X, y_true = make_blobs(n_samples=1000, centers=5, cluster_std=0.60, random_state=42)\n",
    "    ...\n",
    "\n",
    "ans6a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 6-b. Briefly explain how the choice of initialization (random vs. k-means++) influences the convergence speed and the quality of the clustering results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### 7-a. Determining Optimal K Using the Elbow Method  \n",
    "\n",
    "- In this question, you will explore how to select the optimal number of clusters using the **Elbow Method**.  \n",
    "- Run **K-Means clustering** on a synthetic dataset for different values of `k` (from 1 to 10), using the **k-means++ initialization** strategy.  \n",
    "- For each `k`:  \n",
    "  - Record the Within-Cluster Sum of Squares (WCSS).  \n",
    "  - Record the Rand Index score relative to the true labels.  \n",
    "- Based on the results, determine the **optimal `k` value**.  \n",
    "- Return the following values (in this order):  \n",
    "  - `optimal_k`: the optimal number of clusters, \n",
    "  - `rand_scores_list`: the list of Rand Index scores, \n",
    "  - `wcss_list`: the list of WCSS values, \n",
    "  - `k_values`: the list of k values, \n",
    "  - `wcss_list`: the list of WCSS values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import warnings\n",
    "\n",
    "def ans7a():\n",
    "    # Generated synthetic dataset with 5 clusters\n",
    "    X, y_true = make_blobs(n_samples=1000, centers=5, cluster_std=0.60, random_state=42)\n",
    "    ...\n",
    "\n",
    "optimal_k, rand_scores, wcss, k_values, wcss_list = ans7a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### 7-b. Elbow Curve\n",
    "\n",
    "- Create a visualization of the **Elbow Curve** (WCSS vs. k). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q4a": {
     "name": "q4a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_ans4a(ans4a):\n...     expected_centers = np.array([[-2.63, 9.04], [-6.88, -6.98], [4.75, 2.01]])\n...     expected_wcss = 566.86\n...     (centers, labels, wcss, X) = ans4a()\n...     assert centers.shape == (3, 2)\n...     assert labels.shape == (300,)\n...     assert set(labels) == {0, 1, 2}\n...     assert np.allclose(centers, expected_centers, atol=0.1)\n...     assert np.isclose(wcss, expected_wcss, atol=1)\n>>> test_ans4a(ans4a)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_ans5a(ans5a):\n...     (pred, true, ari, X, y_true, centers) = ans5a()\n...     assert len(pred) == 300\n...     assert len(true) == 300\n...     assert set(pred) == {0, 1}\n...     assert set(true) == {0, 1}\n...     assert 0 <= ari <= 1\n...     assert np.isclose(ari, 0.28, atol=0.05)\n>>> test_ans5a(ans5a)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import warnings\n>>> def test_ans6a(ans6a):\n...     with warnings.catch_warnings():\n...         warnings.simplefilter('ignore')\n...         (n_iter_random, n_iter_kpp, wcss_random, wcss_kpp) = ans6a()\n...     assert n_iter_random == 6\n...     assert n_iter_kpp == 2\n...     assert abs(wcss_random - 698.77) < 1\n...     assert abs(wcss_kpp - 698.77) < 1\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7a": {
     "name": "q7a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> import warnings\n>>> def test_ans7a(ans7a):\n...     with warnings.catch_warnings():\n...         warnings.simplefilter('ignore')\n...         (k, rand_scores, wcss, k_values, wcss_list) = ans7a()\n...     assert isinstance(k, int)\n...     assert len(rand_scores) == 10 and len(wcss) == 10\n...     assert k == 5\n...     assert np.allclose(wcss, [57466, 27529, 6145, 1891, 698, 644, 591, 537, 503, 451], atol=10)\n...     assert wcss[0] > wcss[-1]\n...     assert wcss[0] > 57000\n...     assert np.allclose(rand_scores[1:], [0.21, 0.61, 0.78, 0.99, 0.93, 0.86, 0.78, 0.7, 0.61], atol=0.05)\n...     assert wcss[0] / wcss[1] > 2\n...     assert wcss[2] / wcss[3] > 3\n...     assert wcss[8] - wcss[9] > 50\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
